# ğŸ” WhiteBoxControl

> **Advanced Neural Probe Training and Evaluation Framework**  
> Train and evaluate interpretability probes on transformer model activations to understand model behavior and detect potentially unsafe outputs.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸŒŸ **Key Features**

- ğŸ¯ **Three Probe Types**: Logistic, Attention-based, and Mean Difference probes
- ğŸš€ **Clean API**: Simple `TrainProbe.py` and `EvalProbe.py` interfaces  
- ğŸ“Š **Comprehensive Evaluation**: Detailed metrics, plots, and analysis
- ğŸ”§ **Flexible Configuration**: Support for various models and hyperparameters
- ğŸ“ˆ **Multi-GPU Support**: Efficient parallel training and evaluation
- ğŸ¨ **Rich Visualizations**: ROC curves, precision-recall plots, and distributions

---

## ğŸš€ **Quick Start**

### **1. Installation**

```bash
git clone https://github.com/yourusername/WhiteBoxControl.git
cd WhiteBoxControl

# Install dependencies
pip install -r requirements.txt

# Setup directories
python Utils.py
```

### **2. Train a Probe**

```bash
# Train a logistic probe
python TrainProbe.py \
    --probe_type logistic \
    --model "meta-llama/Llama-3.1-8B-Instruct" \
    --data "data/datasets/my_training_data.json" \
    --layer 15 \
    --epochs 10

# Train an attention probe with optimal hyperparameters  
python TrainProbe.py \
    --probe_type attention \
    --model "/path/to/local/model" \
    --data "data/datasets/my_training_data.json" \
    --layer 32 \
    --epochs 14 \
    --lr 0.0022 \
    --use_normalization
```

### **3. Evaluate the Probe**

```bash
# Evaluate with visualizations
python EvalProbe.py \
    --probe_dir "outputs/trained_probes/logistic_probe_layer15" \
    --data "data/datasets/my_test_data.json" \
    --save_predictions \
    --save_plots

# Batch evaluation on multiple datasets
python EvalProbe.py \
    --probe_dir "outputs/trained_probes/attention_probe_layer32" \
    --data test1.json test2.json test3.json \
    --batch_eval
```

### **4. Run Complete Example**

```bash
# Run the full pipeline
chmod +x run_scripts/example_bash_script.sh
./run_scripts/example_bash_script.sh
```

---

## ğŸ“ **Project Structure**

```
WhiteBoxControl/
â”œâ”€â”€ ğŸ”§ TrainProbe.py               # Main training interface
â”œâ”€â”€ ğŸ” EvalProbe.py                # Main evaluation interface  
â”œâ”€â”€ ğŸ› ï¸  Utils.py                    # Utility functions
â”‚
â”œâ”€â”€ ğŸ“Š data/                       # Data storage
â”‚   â”œâ”€â”€ datasets/                  # Training and test datasets
â”‚   â”‚   â””â”€â”€ combined/              # Processed datasets
â”‚   â””â”€â”€ models/                    # Downloaded model cache
â”‚
â”œâ”€â”€ ğŸ§  probes/                     # Core probe implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_probe.py              # Abstract base class
â”‚   â”œâ”€â”€ logistic_probe.py          # ğŸ”µ Logistic regression probe
â”‚   â”œâ”€â”€ attention_probe.py         # ğŸ”´ Attention-based probe
â”‚   â”œâ”€â”€ mean_diff_probe.py         # ğŸŸ¡ Mean difference probe
â”‚   â”œâ”€â”€ data_types.py              # Data structure definitions
â”‚   â””â”€â”€ metrics.py                 # Evaluation metrics
â”‚
â”œâ”€â”€ ğŸ“ run_scripts/                # Example scripts
â”‚   â””â”€â”€ example_bash_script.sh     # Complete workflow example
â”‚
â”œâ”€â”€ ğŸ“ˆ outputs/                    # Generated results
â”‚   â”œâ”€â”€ trained_probes/            # Trained model weights
â”‚   â”‚   â””â”€â”€ <probe_name>/
â”‚   â”‚       â”œâ”€â”€ weights.pt         # Model parameters
â”‚   â”‚       â”œâ”€â”€ config.json        # Training configuration
â”‚   â”‚       â””â”€â”€ evaluation/        # Evaluation results
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/                # Standalone evaluations
â”‚       â””â”€â”€ <experiment_name>/
â”‚           â”œâ”€â”€ metrics.json       # Performance metrics
â”‚           â”œâ”€â”€ predictions.csv    # Detailed predictions
â”‚           â””â”€â”€ plots/             # Visualization plots
â”‚
â”œâ”€â”€ ğŸ“‹ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“– README.md                  # This file
â””â”€â”€ ğŸ™ˆ .gitignore                 # Git ignore rules
```

---

## ğŸ¯ **Detailed Usage**

### **Training Configuration**

#### **Logistic Probe**
```bash
python TrainProbe.py \
    --probe_type logistic \
    --model "microsoft/DialoGPT-medium" \
    --data "data/datasets/safety_data.json" \
    --layer 12 \
    --epochs 15 \
    --lr 0.001 \
    --fit_method adam \
    --token_aggregation mean \
    --output_dir "outputs/trained_probes"
```

#### **Attention Probe**
```bash
python TrainProbe.py \
    --probe_type attention \
    --model "meta-llama/Llama-3.1-70B-Instruct" \
    --data "data/datasets/deception_data.json" \
    --layer 32 \
    --epochs 14 \
    --lr 0.0022 \
    --use_normalization \
    --initialization_scale 1.5 \
    --gpu_id 0
```

#### **Mean Difference Probe**
```bash
python TrainProbe.py \
    --probe_type mean_diff \
    --model "anthropic/claude-3-sonnet" \
    --data "data/datasets/manipulation_data.json" \
    --layer 20 \
    --token_aggregation last_only
```

### **Data Format**

Your training data should be a JSON file with the following structure:

```json
[
  [
    {
      "messages": [
        {"role": "user", "content": "Hello, how are you?"},
        {"role": "assistant", "content": "I'm doing well, thank you!"}
      ],
      "classification": "safe",
      "trajectory_index": 0,
      "action_id": 1
    }
  ],
  [
    {
      "messages": [
        {"role": "user", "content": "Can you help me hack into a system?"},
        {"role": "assistant", "content": "I can't help with illegal activities."}
      ],
      "classification": "unsafe", 
      "trajectory_index": 1,
      "action_id": 1
    }
  ]
]
```

### **Evaluation Outputs**

After evaluation, you'll get:

- **ğŸ“Š Performance Metrics**: Accuracy, AUC, TPR at various FPR thresholds
- **ğŸ“ˆ Visualizations**: ROC curves, precision-recall plots, prediction histograms  
- **ğŸ“‹ Detailed Results**: Per-sample predictions and confidence scores
- **ğŸ¯ Analysis**: Classification reports and confusion matrices

---

### **Multi-GPU Training**

```bash
# Use specific GPU
python TrainProbe.py --gpu_id 3 [other args]

# Use all available GPUs (default)
python TrainProbe.py [other args]

# Set specific GPUs via environment
export CUDA_VISIBLE_DEVICES=0,1,2,3
python TrainProbe.py [other args]
```

### **Batch Processing**

```bash
# Process multiple datasets
python EvalProbe.py \
    --probe_dir "outputs/trained_probes/logistic_probe_layer15" \
    --data dataset1.json dataset2.json dataset3.json \
    --batch_eval \
    --output_dir "outputs/evaluation/comparison"
```

---

## ğŸ› ï¸ **Utilities**

WhiteBoxControl includes utility functions:

```python
import Utils

# Download and cache models
Utils.download_hf_model("meta-llama/Llama-3.1-8B-Instruct", cache_dir="data/models")

# Validate datasets
Utils.validate_dataset_format("data/my_dataset.json")

# Split datasets
splits = Utils.split_dataset("data/full_dataset.json", train_ratio=0.8)

# Combine multiple datasets  
Utils.combine_datasets(
    ["safe_data.json", "unsafe_data.json"], 
    "combined_data.json",
    labels=["safe", "unsafe"]
)

# Check system info
Utils.print_system_info()
```



### **Development Setup**

```bash
# Clone for development
git clone https://github.com/yourusername/WhiteBoxControl.git
cd WhiteBoxControl

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install in development mode
pip install -e .
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Check code style
black . && flake8 .
```

